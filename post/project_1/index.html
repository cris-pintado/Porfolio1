<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Project 1: Analytical Review for Consolidating Habitat Mapping
      Within Europe: Insights from Portugal, Spain, and France | Christopher Pintado</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="The goal of this project was to produce a prototype of harmonized of legend according to the Annex I of HD of of three selected
conterminous European countries.">
    <meta name="generator" content="Hugo 0.76.5" />
    
    
      <META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">
    

    
    
      <link href="https://cris-pintado.github.io/Porfolio1/dist/css/app.4fc0b62e4b82c997bb0041217cd6b979.css" rel="stylesheet">
    

    

    
      

    

    
    
    <meta property="og:title" content="Project 1: Analytical Review for Consolidating Habitat Mapping
      Within Europe: Insights from Portugal, Spain, and France" />
<meta property="og:description" content="The goal of this project was to produce a prototype of harmonized of legend according to the Annex I of HD of of three selected
conterminous European countries." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cris-pintado.github.io/Porfolio1/post/project_1/" />
<meta property="article:published_time" content="2021-04-01T00:00:00+00:00" />
<meta property="article:modified_time" content="2021-04-01T00:00:00+00:00" />
<meta itemprop="name" content="Project 1: Analytical Review for Consolidating Habitat Mapping
      Within Europe: Insights from Portugal, Spain, and France">
<meta itemprop="description" content="The goal of this project was to produce a prototype of harmonized of legend according to the Annex I of HD of of three selected
conterminous European countries.">
<meta itemprop="datePublished" content="2021-04-01T00:00:00+00:00" />
<meta itemprop="dateModified" content="2021-04-01T00:00:00+00:00" />
<meta itemprop="wordCount" content="1682">



<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Project 1: Analytical Review for Consolidating Habitat Mapping
      Within Europe: Insights from Portugal, Spain, and France"/>
<meta name="twitter:description" content="The goal of this project was to produce a prototype of harmonized of legend according to the Annex I of HD of of three selected
conterminous European countries."/>

	
  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  
  
  <header class="cover bg-top" style="background-image: url('https://cris-pintado.github.io/Porfolio1/images/Project_1/inspecting.png');">
    <div class="pb3-m pb6-l bg-black-60">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="https://cris-pintado.github.io/Porfolio1/" class="f3 fw2 hover-white no-underline white-90 dib">
      
        Christopher Pintado
      
    </a>
    <div class="flex-l items-center">
      

      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="https://cris-pintado.github.io/Porfolio1/about/" title="About Me page">
              About Me
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="https://cris-pintado.github.io/Porfolio1/contact/" title="Contact Information page">
              Contact Information
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="https://cris-pintado.github.io/Porfolio1/post/" title="Projects page">
              Projects
            </a>
          </li>
          
        </ul>
      
      







<a href="https://www.linkedin.com/in/pintado1999/" target="_blank" class="link-transition linkedin link dib z-999 pt3 pt0-l mr1" title="LinkedIn link" rel="noopener" aria-label="follow on LinkedIn——Opens in a new window">
  <svg  height="32px"  style="enable-background:new 0 0 65 65;" version="1.1" viewBox="0 0 65 65" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
  <path d="M50.837,48.137V36.425c0-6.275-3.35-9.195-7.816-9.195  c-3.604,0-5.219,1.983-6.119,3.374V27.71h-6.79c0.09,1.917,0,20.427,0,20.427h6.79V36.729c0-0.609,0.044-1.219,0.224-1.655  c0.49-1.22,1.607-2.483,3.482-2.483c2.458,0,3.44,1.873,3.44,4.618v10.929H50.837z M22.959,24.922c2.367,0,3.842-1.57,3.842-3.531  c-0.044-2.003-1.475-3.528-3.797-3.528s-3.841,1.524-3.841,3.528c0,1.961,1.474,3.531,3.753,3.531H22.959z M34,64  C17.432,64,4,50.568,4,34C4,17.431,17.432,4,34,4s30,13.431,30,30C64,50.568,50.568,64,34,64z M26.354,48.137V27.71h-6.789v20.427  H26.354z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>


<a href="https://github.com/cris-pintado" target="_blank" class="link-transition github link dib z-999 pt3 pt0-l mr1" title="Github link" rel="noopener" aria-label="follow on Github——Opens in a new window">
  <svg  height="32px"  style="enable-background:new 0 0 512 512;" version="1.1" viewBox="0 0 512 512" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
  <path d="M256,32C132.3,32,32,134.8,32,261.7c0,101.5,64.2,187.5,153.2,217.9c11.2,2.1,15.3-5,15.3-11.1   c0-5.5-0.2-19.9-0.3-39.1c-62.3,13.9-75.5-30.8-75.5-30.8c-10.2-26.5-24.9-33.6-24.9-33.6c-20.3-14.3,1.5-14,1.5-14   c22.5,1.6,34.3,23.7,34.3,23.7c20,35.1,52.4,25,65.2,19.1c2-14.8,7.8-25,14.2-30.7c-49.7-5.8-102-25.5-102-113.5   c0-25.1,8.7-45.6,23-61.6c-2.3-5.8-10-29.2,2.2-60.8c0,0,18.8-6.2,61.6,23.5c17.9-5.1,37-7.6,56.1-7.7c19,0.1,38.2,2.6,56.1,7.7   c42.8-29.7,61.5-23.5,61.5-23.5c12.2,31.6,4.5,55,2.2,60.8c14.3,16.1,23,36.6,23,61.6c0,88.2-52.4,107.6-102.3,113.3   c8,7.1,15.2,21.1,15.2,42.5c0,30.7-0.3,55.5-0.3,63c0,6.1,4,13.3,15.4,11C415.9,449.1,480,363.1,480,261.7   C480,134.8,379.7,32,256,32z"/>
</svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>








    </div>
  </div>
</nav>

      <div class="tc-l pv6 ph3 ph4-ns">
        
          <h1 class="f2 f1-l fw2 white-90 mb0 lh-title">Project 1: Analytical Review for Consolidating Habitat Mapping
      Within Europe: Insights from Portugal, Spain, and France</h1>
          
            <h2 class="fw1 f5 f3-l white-80 measure-wide-l center lh-copy mt3 mb4">
              The goal of this project was to produce a prototype of harmonized of legend according to the Annex I of HD of of three selected
conterminous European countries.
            </h2>
          
        
      </div>
    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked">
          
        PROJECTS
      </aside>
      




  <div id="sharing" class="mt3">

    
    <a href="https://www.facebook.com/sharer.php?u=https://cris-pintado.github.io/Porfolio1/post/project_1/" class="facebook no-underline" aria-label="share on Facebook">
      <svg height="32px"  style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M28.765,50.32h6.744V33.998h4.499l0.596-5.624h-5.095  l0.007-2.816c0-1.466,0.14-2.253,2.244-2.253h2.812V17.68h-4.5c-5.405,0-7.307,2.729-7.307,7.317v3.377h-3.369v5.625h3.369V50.32z   M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;"/></svg>

    </a>

    
    
    <a href="https://twitter.com/share?url=https://cris-pintado.github.io/Porfolio1/post/project_1/&amp;text=Project%201:%20Plant%20Disease%20Estimator" class="twitter no-underline" aria-label="share on Twitter">
      <svg height="32px"  style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167,22.283c-2.619,0.953-4.274,3.411-4.086,6.101  l0.063,1.038l-1.048-0.127c-3.813-0.487-7.145-2.139-9.974-4.915l-1.383-1.377l-0.356,1.017c-0.754,2.267-0.272,4.661,1.299,6.271  c0.838,0.89,0.649,1.017-0.796,0.487c-0.503-0.169-0.943-0.296-0.985-0.233c-0.146,0.149,0.356,2.076,0.754,2.839  c0.545,1.06,1.655,2.097,2.871,2.712l1.027,0.487l-1.215,0.021c-1.173,0-1.215,0.021-1.089,0.467  c0.419,1.377,2.074,2.839,3.918,3.475l1.299,0.444l-1.131,0.678c-1.676,0.976-3.646,1.526-5.616,1.568  C19.775,43.256,19,43.341,19,43.405c0,0.211,2.557,1.397,4.044,1.864c4.463,1.377,9.765,0.783,13.746-1.568  c2.829-1.673,5.657-5,6.978-8.221c0.713-1.716,1.425-4.851,1.425-6.354c0-0.975,0.063-1.102,1.236-2.267  c0.692-0.678,1.341-1.419,1.467-1.631c0.21-0.403,0.188-0.403-0.88-0.043c-1.781,0.636-2.033,0.551-1.152-0.402  c0.649-0.678,1.425-1.907,1.425-2.267c0-0.063-0.314,0.042-0.671,0.233c-0.377,0.212-1.215,0.53-1.844,0.72l-1.131,0.361l-1.027-0.7  c-0.566-0.381-1.361-0.805-1.781-0.932C39.766,21.902,38.131,21.944,37.167,22.283z M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4  s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/></svg>

    </a>

    
    <a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://cris-pintado.github.io/Porfolio1/post/project_1/&amp;title=Project%201:%20Plant%20Disease%20Estimator" class="linkedin no-underline" aria-label="share on LinkedIn">
      <svg  height="32px"  style="enable-background:new 0 0 65 65;" version="1.1" viewBox="0 0 65 65" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
  <path d="M50.837,48.137V36.425c0-6.275-3.35-9.195-7.816-9.195  c-3.604,0-5.219,1.983-6.119,3.374V27.71h-6.79c0.09,1.917,0,20.427,0,20.427h6.79V36.729c0-0.609,0.044-1.219,0.224-1.655  c0.49-1.22,1.607-2.483,3.482-2.483c2.458,0,3.44,1.873,3.44,4.618v10.929H50.837z M22.959,24.922c2.367,0,3.842-1.57,3.842-3.531  c-0.044-2.003-1.475-3.528-3.797-3.528s-3.841,1.524-3.841,3.528c0,1.961,1.474,3.531,3.753,3.531H22.959z M34,64  C17.432,64,4,50.568,4,34C4,17.431,17.432,4,34,4s30,13.431,30,30C64,50.568,50.568,64,34,64z M26.354,48.137V27.71h-6.789v20.427  H26.354z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>

    </a>
  </div>


      <h1 class="f1 athelas mt3 mb1">Project 1: Analytical Review for Consolidating Habitat Mapping
      Within Europe: Insights from Portugal, Spain, and France</h1>
      
      
      <time class="f6 mv4 dib tracked" datetime="2021-04-01T00:00:00Z">September 2024</time>

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links nested-img mid-gray pr4-l w-two-thirds-l"><h1 id="leaf_disease_estimator">Leaf_disease_estimator</h1>
<p><strong><a href="https://cris-pintado.github.io/Porfolio1/image/TFM_9-1-2024.pdf">See full article here</a></strong>.</p>
<h2 id="an-app-to-estimate-leaf-spot-disease-scores-on-cereals-"><strong>An App to estimate leaf spot disease scores on cereals 🌾</strong></h2>
<ul>
<li>
<!--Describir un poco de los producido-->
<p><strong>Description</strong>: The goal of this project was to use CNNs to predict the disease rating, from an input image.</p>
</li>
<li>
<p><strong>Introduction</strong>: Plant diseases have long been one of the major threats to food security because they dramatically reduce the crop yield and compromises its quality. Accurate and precise estimation of the disease level has been a difficult challenge. Nonetheless, over the last years, the depelopment of deep learning approaches and in particular of CNNs, has brought significant progress to the automatic estimation of crop disease levels. These advances in image processing will definetely benefit the <strong>decission making</strong> process for farmers and they will pave the way towards more sustainable agro-systems 🔄. Furthermore, such automatized phenotypic tools will play an important role in the <strong>crop improvement programs</strong> where plant phenotyping has largely been the most labour-intensive part of the process.</p>
</li>
<li>
<p><strong>Challenges</strong>. There are two <strong>main issues</strong> that remain with the use of CNNs:</p>
<ul>
<li>
<p>Require <strong>large datasets</strong>.</p>
</li>
<li>
<p>Database construction can be <strong>labour intensive</strong> (all pictures need to be properly labeled with the correct disease).</p>
<blockquote>
<p>Hence, there is an increasing interest in developing well-performing models by using <strong>small picture datasets</strong>.</p>
</blockquote>
<blockquote>
<p>In this project I will use a very small picture dataset (144 pictures total) and try to overcome the issues associated.</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<h3 id="code-and-resources-used">Code and Resources Used</h3>
<ul>
<li>
<p><strong>Python Version</strong>: 3.7</p>
</li>
<li>
<p><strong>Packages</strong>: pandas (1.1.3), numpy (1.19.5), sklearn (0.24.1), os, glob, scipy (1.6.0), matplotlib (3.3.4), plotly (4.14.3), streamlit (0.79.0), PIL, cv2 (4.4.0), pickle, keras (2.4.0).</p>
</li>
</ul>
<h2 id="schema-of-the-approach">Schema of the approach:</h2>
<p><strong>1. Review the literature</strong> 📚:</p>
<p><strong>2. Inspect the target variable and the picture dataset</strong> 🖼📊:</p>
<p><strong>3. Choose an approach for our given problem 🎯</strong>:</p>
<p>It is a good idea to start from a simple approach and improve it from there.</p>
<p><strong>4. Resize pictures, split the dataset and use data augmentation</strong>:</p>
<p><strong>5. Adapting the Inception_V3 Architecture for our regression problem</strong>:</p>
<p><strong>6. Propose possible improvements</strong>:</p>
<p><strong>7. Leaf segmentation</strong>:</p>
<p><strong>8. Model Deployment</strong></p>
<hr>
<h2 id="1-review-of-the-literature">1. Review of the literature</h2>
<p>Before jumping into coding, it is important to find out what other image-based methods have been used for similar tasks as the one presented here.
This is key to choose a good approach for our particular problem.</p>
<ul>
<li>
<p>Leaf spot diseases result in different leaf symptoms which include <a href="https://www.ag.ndsu.edu/publications/crops/fungal-leaf-spot-diseases-of-wheat-tan-spot-septoria-stagonospora-nodorum-blotch-and-septoria-tritici-blotch">necrotic and spot-like leaf lessions</a>.</p>
</li>
<li>
<p>The problem plant disease recognition has been historically based on conventional machine learning techniques such as Support Vector Machines, Multilayer Perceptron Neural Networks and Decision Trees. However, the prevailing approach has shifted to the application of deep learning concepts, with focus on <strong>Convolutional Neural Networks (CNNs)</strong> [<a href="https://www.sciencedirect.com/science/article/abs/pii/S0168169918304617">Reference 2018</a>], [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6413718/">Reference 2019</a>], <a href="https://link.springer.com/article/10.1007/s41870-020-00437-5">Reference 2020</a>].</p>
</li>
<li>
<p>CNNs have been used multiple times for <strong>regression</strong> [<a href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-020-00698-y">Reference 2020</a>], but some considerations need to be made for a regression use case.</p>
<ul>
<li>We need to ensure that your final fully connected layer uses a linear activation function instead of sigmoid.</li>
<li>There have been studies that showed a better model performance using a classification-then-regression trick: A first network only determines if y should be in one of, say, 10 bins. A second network then performs the regression task [<a href="https://lmb.informatik.uni-freiburg.de/Publications/2015/FDB15/image_orientation.pdf">Reference 2015</a>].</li>
</ul>
</li>
<li>
<p>There have been a number of <strong>interesting approaches</strong> to tackle the issue with dealing with small picture datasets which include:</p>
<ul>
<li>CNN (InceptionV3 and AlexNet) for feature extraction followed by SVM for classification [<a href="https://www.researchgate.net/publication/334743331_RICE_PLANT_DISEASE_CLASSIFICATION_USING_TRANSFER_LEARNING_OF_DEEP_CONVOLUTION_NEURAL_NETWORK">Reference (2019)</a>], [<a href="https://www.sciencedirect.com/science/article/abs/pii/S0168169920302544">Reference (2020)</a>].</li>
<li>Siamese Networks (InceptionV3) [<a href="https://www.sciencedirect.com/science/article/abs/pii/S0168169920302544">Ref 2020</a>]</li>
</ul>
</li>
</ul>
<hr>
<h2 id="2-inspect-the-target-variable-severity-of-infection-and-the-picture-dataset">2. Inspect the target variable (severity of infection) and the picture dataset.</h2>
<ul>
<li>As we can observe in the plot below, we will be dealing with a <strong>challenging dataset</strong>, not only due to its small size, but also the apparent class imbalance.</li>
</ul>
<figure>
    <img src="https://cris-pintado.github.io/Porfolio1/images/Project_1/target_variable.png"/> 
</figure>

<ul>
<li>Inspecting the picture dataset we can see that these are rectangular pictures with a <strong>ratio of 1.33 and a size of 1390 x 1038 pixels</strong>.</li>
</ul>
<h2 id="hahahugoshortcode-s1-hbhb"><figure>
    <img src="https://cris-pintado.github.io/Porfolio1/images/Project_1/inspecting.png"/> 
</figure>
</h2>
<h2 id="3-choose-an-approach-for-our-given-problem-">3. Choose an approach for our given problem 🎯:</h2>
<ul>
<li>An important aspect to consider is the <strong>image size and aspect ratio</strong>:</li>
</ul>
<p>It is important to note that tradititonal <strong>CNNs are not scale-invariant</strong>, hence, we should re-scale the images to an <strong>square format</strong>. There are three main approaches for this:</p>
<p><em>Note: Another option would be to make our own network architecture taking image features into account by using asymmetric pooling and convolutions</em></p>
<blockquote>
<ol>
<li><strong>Padding the image with 0s</strong>: This approach is not the most recommended, specially having large picture size such in our particular case. <a href="https://datascience.stackexchange.com/questions/30819/image-resizing-and-padding-for-cnn">Read here</a> and <a href="https://stackoverflow.com/questions/47697622/cnn-image-resizing-vs-padding-keeping-aspect-ratio-or-not/49882055#49882055">here</a>.</li>
</ol>
</blockquote>
<blockquote>
<ol start="2">
<li><strong>Resize the pictures with out preserving the aspect ratio</strong> (no cropping involved).</li>
</ol>
</blockquote>
<blockquote>
<ol start="3">
<li><strong>Crop down the picture to preserve the aspect ratio of the picture</strong>. This allows us to maintain a consistent aspect ratio, which helps the CNN to learn more discriminative, consistent features, but potentially loosing information after the cropping process.</li>
</ol>
</blockquote>
<p>The third approach could be detrimental because there may be a direct association between larger leaves and disease severity. Hence, for our use case, the second approach should be preferred.</p>
<ul>
<li>My approaches will rely on <strong>four main strategies</strong> thave been shown to be very effective when faced with <strong>small picture datasets</strong>:</li>
</ul>
<blockquote>
<ol>
<li><strong>Data Augmentation</strong>: used to generate new training samples from the original ones by applying random modifications and perturbations such that the classes labels are not changed.</li>
</ol>
</blockquote>
<blockquote>
<ol start="2">
<li><strong>Transfer Learning by placing a new set of FC layers on top of the CNN</strong>, and then fine-tuning these weights.</li>
</ol>
</blockquote>
<blockquote>
<ol start="3">
<li><strong>Cross-Validation</strong></li>
</ol>
</blockquote>
<blockquote>
<ol start="4">
<li><strong>Transfer Learning by treating networks as arbitrary feature extractors</strong>.</li>
</ol>
</blockquote>
<ul>
<li>Finally, is also worth noticing that our dataset is somewhat similar to a famous benchmark dataset extensively used in a <strong>Leaf Segmentation Challenge (LSC)</strong> from the Computer Vision Problems in Plant Phenotyping platform (<strong><a href="https://www.plant-phenotyping.org/CVPPP2020">CVPPP</a></strong>). Similarly, all images are taken from the top and have uniform backgrounds.
Hence, a transfer learning approach could be also used taking advantage of one of the state of the art models trained on the CVPPP dataset (i.e. <strong><a href="https://github.com/csiro-robotics/UPGen">Mask R-CNN</a></strong>). For this, we should have segmented versions of our pictures. A possibility that I mention in the last section (possible improvements).</li>
</ul>
<hr>
<h2 id="4-resize-pictures-and-use-data-augmentation">4. Resize pictures and use data augmentation.</h2>
<ul>
<li>Data Split: Stratified data split was performed to preserve equal distribution of disease severity accross sets.
*Note: <code>MinMaxScaler</code> was used after splitting to scale the target variable between 0 and 1.</li>
</ul>
<figure>
    <img src="https://cris-pintado.github.io/Porfolio1/images/Project_1/stratified_split.png"/> 
</figure>

<ul>
<li>Data Augmentation: Rotation, vertical and horizontal flippling were used in this case to enhance the train set.</li>
</ul>
<figure>
    <img src="https://cris-pintado.github.io/Porfolio1/images/Project_1/data_augmentation.png"/> 
</figure>

<hr>
<h2 id="5-approach-adapting-the-inception_v3-architecture-for-our-regression-problem">5. Approach: Adapting the Inception_V3 Architecture for our regression problem.</h2>
<ul>
<li><strong>Tranfer learning</strong>: We will exclude the last two layers of the inception_v3 model and include our <code>GlobalAveragePooling2D</code>, <code>Dense</code>, <code>BatchNormalization</code>, <code>Dropout</code> and most importantly a last dense layer with a <code>linear</code> activation fuction to perform the regression.</li>
</ul>
<figure>
    <img src="https://cris-pintado.github.io/Porfolio1/images/Project_1/Inception_v3.png"/> 
</figure>

<blockquote>
<ol>
<li>Train the network with a learning rate of 0.001 with all inception_v3 layers trained in the imagenet dataset frozen.</li>
<li>Unfreeze all layers and fine tune the model with a learning rate = 0.00001.</li>
</ol>
</blockquote>
<figure>
    <img src="https://cris-pintado.github.io/Porfolio1/images/Project_1/Training_raw_scaler.png"/> 
</figure>

<ul>
<li><strong>Model Evaluation</strong>:</li>
</ul>
<p>Mean absolute error(mae) and mean squared error(mse) were used as evaluation metrics. As loss function mse was used to penalize more the large errors.</p>
<pre><code>  Final model -&gt; mae:  0.06, mse:  0.22, loss:  0.06
</code></pre>
<figure>
    <img src="https://cris-pintado.github.io/Porfolio1/images/Project_1/pred_vs_truth.png"/> 
</figure>

<hr>
<h2 id="6-possible-improvements">6. Possible Improvements</h2>
<ul>
<li>
<p><strong>A)</strong> One of the possible set of improvements that can bring a better performance over the first approach is to use one or several of the following techniques to deal <strong>data imbalance</strong>:</p>
<ul>
<li>
<p><strong>Oversampling</strong>: Increasing the number of pictures for the under-represented disease levels. Data augmentation techniques can also be of use here. [<a href="https://towardsdatascience.com/deep-learning-unbalanced-training-data-solve-it-like-this-6c528e9efea6">Reference</a>].</p>
</li>
<li>
<p><strong>Synthetic Minority Over-sampling Technique(SMOTE)</strong>. This would require to increase the number of samples for the minority classes. <a href="https://github.com/scikit-learn-contrib/imbalanced-learn/issues/27">See github issue</a></p>
</li>
<li>
<p><strong>Adjust the weights</strong> to the classess.</p>
</li>
<li>
<p><strong>Define our custom loss-function</strong>.</p>
</li>
</ul>
</li>
<li>
<p><strong>B) Increase the picture dataset</strong>:</p>
<ul>
<li>
<p>As suggested above, improving the number of labeled pictures (in particular for the minority disease score), would help us to improve the model&rsquo;s performance further.</p>
</li>
<li>
<p>Furthermore, it would be interesting to complement the pictures taken from above with <strong>side pictures</strong> of the plants.</p>
</li>
</ul>
</li>
<li>
<p><strong>C)</strong> Try other <strong>image-preprocessing methods</strong>:</p>
<ul>
<li>
<p><strong>Background-removal</strong></p>
</li>
<li>
<p><strong>Semantic segmentation</strong></p>
</li>
<li>
<p><strong>Instance Segmentation</strong>:</p>
<ul>
<li>
<ol>
<li>Mask R-CNN: To automatically segment and construct pixel-wise masks for our pictures. For this we could use the <a href="https://github.com/matterport/Mask_RCNN">matterport repository</a>.</li>
</ol>
</li>
<li>
<ol start="2">
<li>Perform transfer learning using a <a href="https://github.com/csiro-robotics/UPGen">state-of-the-art</a> model used for leaf phenotyping.</li>
</ol>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>D)</strong> Perform <strong>cross-validation</strong> for hyper-parameter tunning.</p>
</li>
<li>
<p><strong>E)</strong> <strong>Try other algorithms</strong></p>
<ul>
<li><strong>Other CNN architectures</strong>: i.e Xception, EfficientNetB7, Resnet&hellip;</li>
<li><strong>Transformers</strong>: They represent a breakthrough in deep neural network architectures that has recently unlocked dramatic advances across many areas of AI, including <a href="https://arxiv.org/abs/2010.11929">computer vision</a>.</li>
</ul>
</li>
</ul>
<hr>
<h2 id="7-leaf-segmentation">7. Leaf segmentation</h2>
<p>In an attempt to improve the model performance I segmented the pictures using this <a href="https://github.com/YaredTaddese/leaf-image-segmentation">repository on github</a>.</p>
<p><em>Note: In this case the pictures have a white background (only some debry is present in the background), so the effects of the segmentation may not have a large effect on the model performance</em></p>
<p>See below an example of segmented images:</p>
<figure>
    <img src="https://cris-pintado.github.io/Porfolio1/images/Project_1/inspecting_segmented.png"/> 
</figure>

<p>When we compare the training using segmented and the original rgb pictures we see a quicker drop on the <code>val_loss</code> (mse), which may be the result of a more focused training on the leaf-related features, which we expect to be associated to disease severity.</p>
<figure>
    <img src="https://cris-pintado.github.io/Porfolio1/images/Project_1/rgb_vs_segmented.png"/> 
</figure>

<hr>
<h2 id="8-oversampling-of-the-minority-classes-with-data-augmentation">8. Oversampling of the minority classes with data augmentation</h2>
<p>In this section I will use data augmentation to oversample the minority classes. For this I will create a train, val and test folders, and apply data augmentation on the train and val sets.</p>
<ul>
<li>Label target variable distribution before data augmentation (train + val + test sets):</li>
</ul>
<figure>
    <img src="https://cris-pintado.github.io/Porfolio1/images/Project_1/count_plot_before.png"/> 
</figure>

<ul>
<li>Label target variable distribution after data augmentation (train + val sets):</li>
</ul>
<figure>
    <img src="https://cris-pintado.github.io/Porfolio1/images/Project_1/count_plot_after.png"/> 
</figure>

<ul>
<li>Target distribution on the three sets:</li>
</ul>
<figure>
    <img src="https://cris-pintado.github.io/Porfolio1/images/Project_1/stratified_split_aug.png"/> 
</figure>

<ul>
<li>Model training:</li>
</ul>
<figure>
    <img src="https://cris-pintado.github.io/Porfolio1/images/Project_1/Training_marked_seg_aug.png"/> 
</figure>

<ul>
<li>Model evaluation:</li>
</ul>
<p>Mean absolute error(mae) and mean squared error(mse) were used as evaluation metrics. As loss function mse was used to penalize more the large errors.</p>
<pre><code>  Final model -&gt; mae:  0.14, mse:  0.33, loss:  0.14
</code></pre>
<p>In the plot below the predicted values for the test set are ploted against the truth values. The red line represents the perfect prediction, whereas the purple line is the trend line (r<!-- raw HTML omitted -->2<!-- raw HTML omitted -->=0.08)</p>
<figure>
    <img src="https://cris-pintado.github.io/Porfolio1/images/Project_1/pred_vs_truth_aug.png"/> 
</figure>

<p>We can see that we did not improve model&rsquo;s performance on the test set by oversampling the train and val sets using data augmentation on the minority classes.
Indeed, we can expect the model to generalize better accross disease levels, but since the test set is unevenly distributed, performance is hurt.</p>
<hr>
<h2 id="9-model-deployment">9. Model Deployment</h2>
<p>An app was built locally with the <a href="https://streamlit.io/">Streamlit</a> package and deployed it using the <a href="https://dashboard.heroku.com/apps">Heroku</a> framework.</p>
<p>See below couple of screenshots of the app:</p>
<figure>
    <img src="https://cris-pintado.github.io/Porfolio1/images/Project_1/Screenshot_1.png"/> 
</figure>

<figure>
    <img src="https://cris-pintado.github.io/Porfolio1/images/Project_1/Screenshot_2.png"/> 
</figure>

<hr>
<ul class="pa0">
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-near-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="https://cris-pintado.github.io/Porfolio1/" >
    &copy;  Christopher Pintado 2024 
  </a>
    <div>







<a href="https://www.linkedin.com/in/pintado1999/" target="_blank" class="link-transition linkedin link dib z-999 pt3 pt0-l mr1" title="LinkedIn link" rel="noopener" aria-label="follow on LinkedIn——Opens in a new window">
  <svg  height="32px"  style="enable-background:new 0 0 65 65;" version="1.1" viewBox="0 0 65 65" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
  <path d="M50.837,48.137V36.425c0-6.275-3.35-9.195-7.816-9.195  c-3.604,0-5.219,1.983-6.119,3.374V27.71h-6.79c0.09,1.917,0,20.427,0,20.427h6.79V36.729c0-0.609,0.044-1.219,0.224-1.655  c0.49-1.22,1.607-2.483,3.482-2.483c2.458,0,3.44,1.873,3.44,4.618v10.929H50.837z M22.959,24.922c2.367,0,3.842-1.57,3.842-3.531  c-0.044-2.003-1.475-3.528-3.797-3.528s-3.841,1.524-3.841,3.528c0,1.961,1.474,3.531,3.753,3.531H22.959z M34,64  C17.432,64,4,50.568,4,34C4,17.431,17.432,4,34,4s30,13.431,30,30C64,50.568,50.568,64,34,64z M26.354,48.137V27.71h-6.789v20.427  H26.354z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>


<a href="https://github.com/cris-pintado" target="_blank" class="link-transition github link dib z-999 pt3 pt0-l mr1" title="Github link" rel="noopener" aria-label="follow on Github——Opens in a new window">
  <svg  height="32px"  style="enable-background:new 0 0 512 512;" version="1.1" viewBox="0 0 512 512" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
  <path d="M256,32C132.3,32,32,134.8,32,261.7c0,101.5,64.2,187.5,153.2,217.9c11.2,2.1,15.3-5,15.3-11.1   c0-5.5-0.2-19.9-0.3-39.1c-62.3,13.9-75.5-30.8-75.5-30.8c-10.2-26.5-24.9-33.6-24.9-33.6c-20.3-14.3,1.5-14,1.5-14   c22.5,1.6,34.3,23.7,34.3,23.7c20,35.1,52.4,25,65.2,19.1c2-14.8,7.8-25,14.2-30.7c-49.7-5.8-102-25.5-102-113.5   c0-25.1,8.7-45.6,23-61.6c-2.3-5.8-10-29.2,2.2-60.8c0,0,18.8-6.2,61.6,23.5c17.9-5.1,37-7.6,56.1-7.7c19,0.1,38.2,2.6,56.1,7.7   c42.8-29.7,61.5-23.5,61.5-23.5c12.2,31.6,4.5,55,2.2,60.8c14.3,16.1,23,36.6,23,61.6c0,88.2-52.4,107.6-102.3,113.3   c8,7.1,15.2,21.1,15.2,42.5c0,30.7-0.3,55.5-0.3,63c0,6.1,4,13.3,15.4,11C415.9,449.1,480,363.1,480,261.7   C480,134.8,379.7,32,256,32z"/>
</svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>







</div>
  </div>
</footer>

    

  <script src="https://cris-pintado.github.io/Porfolio1/dist/js/app.3fc0f988d21662902933.js"></script>


  </body>
</html>
